#!/bin/bash

# Example script to run DeepRecInfra.
# This allows you to run the neural recommendation models found in models/
# along with the recommendation load generator to measure inference
# tail-latency It does not include the query scheduler which would
# optimize for latency-bounded throughput (inference QPS)

###############################################################################

########## Epoch args ##############
# The total number of inference queries run is product of the number of epochs and number of batches
# Inference querues = nepochs * num_batches
# The number of batches is the unique number of data inputs generated by the data generators
# The number of epochs of epochs determines how many iterations to loop over
# Configuring these parameters is important to getting accurate caching
# behavior based on the use case being modeled
nepochs=32
num_batches=32

epoch_args="--nepochs $nepochs --num_batches $num_batches"

########## Inference engine args ##############

# The number of inference engines determines the unique number of Caffe2
# CPU processes to parallelize queries over
inference_engines=32
caffe2_net_type="async_dag"

engine_args="--inference_engines $inference_engines --caffe2_net_type $caffe2_net_type"

########## Query size args ##############
# Configuration for query sizes.
batch_size_distribution="normal" # number of candidate items in query follows normal distribution
max_mini_batch_size=1024 # maximum number of candidate items queries
avg_mini_batch_size=256 # mean number of canidate items in query
var_mini_batch_size=32 # variation of number of canidate items in query
sub_task_batch_size=32 # per-core query size (number of items processed per-core)

batch_args="--batch_size_distribution $batch_size_distribution --max_mini_batch_size $max_mini_batch_size --avg_mini_batch_size $avg_mini_batch_size --var_mini_batch_size $var_mini_batch_size --sub_task_batch_size $sub_task_batch_size"

########## Accelerator arguments ##############
accel_request_size_thres=256
accel_args="--accel_request_size_thres $accel_request_size_thres --model_accel"

###############################################################################

mkdir -p log

### Wide and Deep Models ###
python DeepRecSys.py $epoch_args $engine_args $batch_args --avg_arrival_rate 25 --queue --config_file "models/configs/wide_and_deep.json" > log/wnd
python DeepRecSys.py $epoch_args $engine_args $batch_args $accel_args --avg_arrival_rate 25 --queue --config_file "models/configs/wide_and_deep.json" > log/wnd_accel

### NCF Model ###
python DeepRecSys.py $epoch_args $engine_args $batch_args --avg_arrival_rate 25 --queue --config_file "models/configs/ncf.json" > log/ncf

### DLRM RM1-3 Models ###
python DeepRecSys.py $epoch_args $engine_args $batch_args --avg_arrival_rate 25 --queue --config_file "models/configs/dlrm_rm1.json" > log/dlrm_rm1
python DeepRecSys.py $epoch_args $engine_args $batch_args --avg_arrival_rate 25 --queue --config_file "models/configs/dlrm_rm2.json" > log/dlrm_rm2
python DeepRecSys.py $epoch_args $engine_args $batch_args --avg_arrival_rate 25 --queue --config_file "models/configs/dlrm_rm3.json" > log/dlrm_rm3

### Deep Interest Model ###
python DeepRecSys.py $epoch_args $engine_args $batch_args --avg_arrival_rate 25 --queue --config_file "models/configs/din.json" > log/din

### Deep Interest Evolution Model ###
python DeepRecSys.py $epoch_args $engine_args $batch_args --avg_arrival_rate 25 --queue --config_file "models/configs/dien.json" > log/dien
